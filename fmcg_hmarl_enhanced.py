import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import deque
import random
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.distributions import Normal
import pandas as pd
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# ê°œì„ ëœ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
class Config:
    # í™˜ê²½ ì„¤ì •
    NUM_EPISODES = 1000
    MAX_STEPS = 100
    NUM_ECHELONS = 4  # ì†Œë§¤ì , RDC, ë„ë§¤ìƒ, ì œì¡°ì—…ì²´
    
    # SAC í•˜ì´í¼íŒŒë¼ë¯¸í„°
    BATCH_SIZE = 256
    BUFFER_SIZE = 100000
    LR_ACTOR = 1e-4  # ë” ì•ˆì •ì ì¸ í•™ìŠµë¥ 
    LR_CRITIC = 1e-4
    LR_ALPHA = 1e-4
    GAMMA = 0.99
    TAU = 0.005
    HIDDEN_DIM = 256
    
    # ê³µê¸‰ë§ ì„¤ì •
    INITIAL_INVENTORY = [100, 200, 300, 500]
    HOLDING_COSTS = [1.0, 0.8, 0.6, 0.4]
    ORDER_COSTS = [2.0, 1.5, 1.2, 1.0]
    SHORTAGE_COSTS = [10.0, 8.0, 6.0, 4.0]
    LEAD_TIMES_MEAN = [1, 2, 3, 4]
    LEAD_TIMES_STD = [0.1, 0.2, 0.3, 0.4]  # ë¦¬ë“œíƒ€ì„ ë³€ë™ì„± ê°ì†Œ
    MAX_ORDER_QTY = [300, 500, 700, 900]  # ìµœëŒ€ ì£¼ë¬¸ëŸ‰ ì¡°ì •
    
    # ê³„ì¸µë³„ ë³´ìƒ ê°€ì¤‘ì¹˜ (ì†Œë§¤ â†’ ì œì¡°ì—…ì²´)
    LAYER_WEIGHTS = [0.4, 0.3, 0.2, 0.1]
    
    # í™˜ê²½ ë³€ë™ì„± ì„¤ì • (Bullwhip ì™„í™”ë¥¼ ìœ„í•´ ê°ì†Œ)
    DEMAND_VOLATILITY = 0.15  # ìˆ˜ìš” ë³€ë™ì„± ê°ì†Œ
    LEAD_TIME_UNCERTAINTY = True
    
    # Bullwhip ê°œì„ ì„ ìœ„í•œ ìƒˆë¡œìš´ ì„¤ì •
    INFORMATION_SHARING = True  # ì •ë³´ ê³µìœ  í™œì„±í™”
    COLLABORATION_BONUS = 5.0   # í˜‘ë ¥ ë³´ë„ˆìŠ¤
    ORDER_SMOOTHING_FACTOR = 0.3  # ì£¼ë¬¸ í‰í™œí™” ê³„ìˆ˜
    BULLWHIP_PENALTY_WEIGHT = 50.0  # Bullwhip í˜ë„í‹° ê°€ì¤‘ì¹˜ ì¦ê°€

config = Config()

class ReplayBuffer:
    """ê°œì„ ëœ ê²½í—˜ ì¬ìƒ ë²„í¼"""
    def __init__(self, capacity: int):
        self.buffer = deque(maxlen=capacity)
        self.priorities = deque(maxlen=capacity)
    
    def push(self, state, action, reward, next_state, done, priority=1.0):
        self.buffer.append((state, action, reward, next_state, done))
        self.priorities.append(priority)
    
    def sample(self, batch_size: int):
        if len(self.buffer) < batch_size:
            return None
        
        priorities = np.array(self.priorities) + 1e-6
        probabilities = priorities / priorities.sum()
        
        indices = np.random.choice(len(self.buffer), batch_size, p=probabilities)
        batch = [self.buffer[idx] for idx in indices]
        
        state, action, reward, next_state, done = map(np.stack, zip(*batch))
        return state, action, reward, next_state, done, indices
    
    def update_priorities(self, indices, priorities):
        for idx, priority in zip(indices, priorities):
            if idx < len(self.priorities):
                self.priorities[idx] = priority
    
    def __len__(self):
        return len(self.buffer)

class Actor(nn.Module):
    """ê°œì„ ëœ SAC Actor ë„¤íŠ¸ì›Œí¬ (ì£¼ë¬¸ í‰í™œí™” í¬í•¨)"""
    def __init__(self, state_dim: int, action_dim: int, hidden_dim: int = 256):
        super(Actor, self).__init__()
        self.fc1 = nn.Linear(state_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.fc3 = nn.Linear(hidden_dim, hidden_dim)
        
        # ì£¼ë¬¸ í‰í™œí™”ë¥¼ ìœ„í•œ ì¶”ê°€ ë ˆì´ì–´
        self.smoothing_layer = nn.Linear(hidden_dim, hidden_dim)
        
        self.fc_mean = nn.Linear(hidden_dim, action_dim)
        self.fc_std = nn.Linear(hidden_dim, action_dim)
        
        # ë“œë¡­ì•„ì›ƒ ì¶”ê°€ (ê³¼ì í•© ë°©ì§€)
        self.dropout = nn.Dropout(0.1)
        
        self.apply(self._init_weights)
        
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            torch.nn.init.xavier_uniform_(m.weight)
            torch.nn.init.constant_(m.bias, 0)
        
    def forward(self, state):
        x = F.relu(self.fc1(state))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        
        # ì£¼ë¬¸ í‰í™œí™”
        x = F.relu(self.smoothing_layer(x))
        
        mean = self.fc_mean(x)
        log_std = self.fc_std(x)
        log_std = torch.clamp(log_std, -10, 2)  # í‘œì¤€í¸ì°¨ ë²”ìœ„ ì œí•œ
        return mean, log_std
    
    def sample(self, state):
        mean, log_std = self.forward(state)
        std = log_std.exp()
        normal = Normal(mean, std)
        x_t = normal.rsample()
        action = torch.tanh(x_t)
        log_prob = normal.log_prob(x_t)
        log_prob -= torch.log(1 - action.pow(2) + 1e-6)
        log_prob = log_prob.sum(1, keepdim=True)
        return action, log_prob, mean

class CentralizedCritic(nn.Module):
    """ì¤‘ì•™ì§‘ì¤‘ì‹ Critic ë„¤íŠ¸ì›Œí¬"""
    def __init__(self, global_state_dim: int, joint_action_dim: int, hidden_dim: int = 256):
        super(CentralizedCritic, self).__init__()
        
        # Q1 ë„¤íŠ¸ì›Œí¬
        self.q1_fc1 = nn.Linear(global_state_dim + joint_action_dim, hidden_dim)
        self.q1_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q1_fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.q1_fc4 = nn.Linear(hidden_dim, 1)
        
        # Q2 ë„¤íŠ¸ì›Œí¬
        self.q2_fc1 = nn.Linear(global_state_dim + joint_action_dim, hidden_dim)
        self.q2_fc2 = nn.Linear(hidden_dim, hidden_dim)
        self.q2_fc3 = nn.Linear(hidden_dim, hidden_dim)
        self.q2_fc4 = nn.Linear(hidden_dim, 1)
        
        # ë“œë¡­ì•„ì›ƒ
        self.dropout = nn.Dropout(0.1)
        
        self.apply(self._init_weights)
    
    def _init_weights(self, m):
        if isinstance(m, nn.Linear):
            torch.nn.init.xavier_uniform_(m.weight)
            torch.nn.init.constant_(m.bias, 0)
    
    def forward(self, global_state, joint_action):
        sa = torch.cat([global_state, joint_action], 1)
        
        # Q1
        q1 = F.relu(self.q1_fc1(sa))
        q1 = self.dropout(q1)
        q1 = F.relu(self.q1_fc2(q1))
        q1 = self.dropout(q1)
        q1 = F.relu(self.q1_fc3(q1))
        q1 = self.q1_fc4(q1)
        
        # Q2
        q2 = F.relu(self.q2_fc1(sa))
        q2 = self.dropout(q2)
        q2 = F.relu(self.q2_fc2(q2))
        q2 = self.dropout(q2)
        q2 = F.relu(self.q2_fc3(q2))
        q2 = self.q2_fc4(q2)
        
        return q1, q2

class MultiAgentSACAgent:
    """ê°œì„ ëœ Multi-Agent SAC ì—ì´ì „íŠ¸"""
    def __init__(self, agent_id: int, local_state_dim: int, global_state_dim: int, 
                 action_dim: int, joint_action_dim: int):
        self.agent_id = agent_id
        self.local_state_dim = local_state_dim
        self.global_state_dim = global_state_dim
        self.action_dim = action_dim
        self.joint_action_dim = joint_action_dim
        
        # ì£¼ë¬¸ ì´ë ¥ ì €ì¥ (í‰í™œí™”ë¥¼ ìœ„í•´)
        self.order_history = deque(maxlen=10)
        
        # Actor ë° Critic ë„¤íŠ¸ì›Œí¬
        self.actor = Actor(local_state_dim, action_dim, config.HIDDEN_DIM)
        self.critic = CentralizedCritic(global_state_dim, joint_action_dim, config.HIDDEN_DIM)
        self.critic_target = CentralizedCritic(global_state_dim, joint_action_dim, config.HIDDEN_DIM)
        
        self.critic_target.load_state_dict(self.critic.state_dict())
        
        # ì˜µí‹°ë§ˆì´ì €
        self.actor_optimizer = optim.Adam(self.actor.parameters(), lr=config.LR_ACTOR)
        self.critic_optimizer = optim.Adam(self.critic.parameters(), lr=config.LR_CRITIC)
        
        # ì ì‘ì  ì˜¨ë„ ì¡°ì ˆ (ë” ë³´ìˆ˜ì )
        self.target_entropy = -action_dim * 0.3
        self.log_alpha = torch.zeros(1, requires_grad=True)
        self.alpha_optimizer = optim.Adam([self.log_alpha], lr=config.LR_ALPHA)
        
        # ê²½í—˜ ì¬ìƒ ë²„í¼
        self.replay_buffer = ReplayBuffer(config.BUFFER_SIZE)
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬
        self.actor_scheduler = optim.lr_scheduler.StepLR(self.actor_optimizer, step_size=100, gamma=0.95)
        self.critic_scheduler = optim.lr_scheduler.StepLR(self.critic_optimizer, step_size=100, gamma=0.95)
    
    @property
    def alpha(self):
        return self.log_alpha.exp()
    
    def select_action(self, local_state, evaluate=False):
        state = torch.FloatTensor(local_state).unsqueeze(0)
        
        if evaluate:
            with torch.no_grad():
                _, _, action = self.actor.sample(state)
                raw_action = action.detach().cpu().numpy()[0]
        else:
            action, _, _ = self.actor.sample(state)
            raw_action = action.detach().cpu().numpy()[0]
        
        # ì£¼ë¬¸ í‰í™œí™” ì ìš©
        if len(self.order_history) > 0:
            recent_avg = np.mean(list(self.order_history)[-3:])
            smoothed_action = (1 - config.ORDER_SMOOTHING_FACTOR) * raw_action + \
                             config.ORDER_SMOOTHING_FACTOR * recent_avg
        else:
            smoothed_action = raw_action
        
        self.order_history.append(smoothed_action)
        return smoothed_action
    
    def update(self, global_experiences):
        """ì „ì—­ ê²½í—˜ì„ ì´ìš©í•œ ì—…ë°ì´íŠ¸"""
        if len(self.replay_buffer) < config.BATCH_SIZE:
            return {}
        
        batch_data = self.replay_buffer.sample(config.BATCH_SIZE)
        if batch_data is None:
            return {}
        
        local_states, actions, rewards, next_local_states, dones, indices = batch_data
        
        local_states = torch.FloatTensor(local_states)
        actions = torch.FloatTensor(actions)
        rewards = torch.FloatTensor(rewards).unsqueeze(1)
        next_local_states = torch.FloatTensor(next_local_states)
        dones = torch.FloatTensor(dones).unsqueeze(1)
        
        # ì „ì—­ ì •ë³´ ìƒì„± (ì‹¤ì œë¡œëŠ” ê³µìœ  ë©”ëª¨ë¦¬ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
        batch_size = local_states.shape[0]
        global_states = torch.randn(batch_size, self.global_state_dim)
        joint_actions = torch.randn(batch_size, self.joint_action_dim)
        next_global_states = torch.randn(batch_size, self.global_state_dim)
        
        # Critic ì—…ë°ì´íŠ¸
        with torch.no_grad():
            next_actions, next_log_probs, _ = self.actor.sample(next_local_states)
            next_joint_actions = joint_actions.clone()
            start_idx = self.agent_id * self.action_dim
            end_idx = (self.agent_id + 1) * self.action_dim
            next_joint_actions[:, start_idx:end_idx] = next_actions
            
            q1_next, q2_next = self.critic_target(next_global_states, next_joint_actions)
            q_next = torch.min(q1_next, q2_next) - self.alpha * next_log_probs
            q_target = rewards + config.GAMMA * (1 - dones) * q_next
        
        q1, q2 = self.critic(global_states, joint_actions)
        critic_loss = F.mse_loss(q1, q_target) + F.mse_loss(q2, q_target)
        
        # TD ì˜¤ì°¨ ì—…ë°ì´íŠ¸
        td_errors = torch.abs(q1 - q_target).detach().cpu().numpy().flatten()
        self.replay_buffer.update_priorities(indices, td_errors + 1e-6)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.critic.parameters(), 1.0)
        self.critic_optimizer.step()
        
        # Actor ì—…ë°ì´íŠ¸
        new_actions, log_probs, _ = self.actor.sample(local_states)
        new_joint_actions = joint_actions.clone()
        new_joint_actions[:, start_idx:end_idx] = new_actions
        
        q1_new, q2_new = self.critic(global_states, new_joint_actions)
        q_new = torch.min(q1_new, q2_new)
        actor_loss = (self.alpha * log_probs - q_new).mean()
        
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.actor.parameters(), 1.0)
        self.actor_optimizer.step()
        
        # Alpha ì—…ë°ì´íŠ¸
        alpha_loss = -(self.log_alpha * (log_probs + self.target_entropy).detach()).mean()
        
        self.alpha_optimizer.zero_grad()
        alpha_loss.backward()
        self.alpha_optimizer.step()
        
        # íƒ€ê²Ÿ ë„¤íŠ¸ì›Œí¬ ì—…ë°ì´íŠ¸
        for target_param, param in zip(self.critic_target.parameters(), self.critic.parameters()):
            target_param.data.copy_(config.TAU * param.data + (1 - config.TAU) * target_param.data)
        
        # í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
        self.actor_scheduler.step()
        self.critic_scheduler.step()
        
        return {
            'critic_loss': critic_loss.item(),
            'actor_loss': actor_loss.item(),
            'alpha_loss': alpha_loss.item(),
            'alpha': self.alpha.item()
        }

class EnhancedFMCGSupplyChain:
    """ê°œì„ ëœ FMCG ë‹¤ê³„ì¸µ ê³µê¸‰ë§ í™˜ê²½ (Bullwhip Effect ì™„í™”)"""
    def __init__(self, scenario='normal'):
        self.num_echelons = config.NUM_ECHELONS
        self.echelon_names = ['Retail', 'RDC', 'Wholesaler', 'Manufacturer']
        self.scenario = scenario
        
        # ìƒíƒœ ì°¨ì›
        self.local_state_dim = 12  # í™•ì¥ëœ ìƒíƒœ (ì •ë³´ ê³µìœ  í¬í•¨)
        self.global_state_dim = self.num_echelons * self.local_state_dim
        self.action_dim = 1
        self.joint_action_dim = self.num_echelons * self.action_dim
        
        # ì •ë³´ ê³µìœ ë¥¼ ìœ„í•œ ê¸€ë¡œë²Œ ë©”íŠ¸ë¦­
        self.global_demand_forecast = deque(maxlen=20)
        self.global_inventory_ratio = 0.0
        self.supply_chain_efficiency = 0.0
        
        self._configure_scenario()
        self.reset()
    
    def _configure_scenario(self):
        """ì‹œë‚˜ë¦¬ì˜¤ë³„ í™˜ê²½ ì„¤ì •"""
        if self.scenario == 'high_volatility':
            self.demand_volatility = 0.25
            self.lead_time_uncertainty = 1.2
        elif self.scenario == 'seasonal':
            self.demand_volatility = 0.2
            self.lead_time_uncertainty = 1.1
            self.seasonal_amplitude = 0.3
        else:  # normal
            self.demand_volatility = config.DEMAND_VOLATILITY
            self.lead_time_uncertainty = 1.0
    
    def reset(self):
        """í™˜ê²½ ì´ˆê¸°í™”"""
        self.current_step = 0
        self.inventories = np.array(config.INITIAL_INVENTORY, dtype=np.float32)
        
        # ìˆ˜ìš” íˆìŠ¤í† ë¦¬ ì´ˆê¸°í™”
        self.demand_history = deque(maxlen=50)
        self.order_history = [deque(maxlen=20) for _ in range(self.num_echelons)]
        
        # ì´ˆê¸° ìˆ˜ìš” ìƒì„±
        for _ in range(20):
            demand = self._generate_demand()
            self.demand_history.append(demand)
            self.global_demand_forecast.append(demand)
        
        # ë¦¬ë“œíƒ€ì„ ë° ìš´ì†¡ í ì´ˆê¸°í™”
        self.current_lead_times = self._sample_lead_times()
        self.in_transit = [deque([0] * max(1, int(lt))) for lt in self.current_lead_times]
        
        self.previous_orders = np.zeros(self.num_echelons)
        
        # ì„±ê³¼ ì¶”ì 
        self.total_costs = []
        self.inventory_levels = []
        self.shortage_events = []
        self.bullwhip_metrics = []
        self.collaboration_scores = []
        
        return self._get_observations()
    
    def _sample_lead_times(self):
        """ë™ì  ë¦¬ë“œíƒ€ì„ ìƒ˜í”Œë§ (ë³€ë™ì„± ê°ì†Œ)"""
        if config.LEAD_TIME_UNCERTAINTY:
            lead_times = []
            for i in range(self.num_echelons):
                # ë” ì•ˆì •ì ì¸ ë¦¬ë“œíƒ€ì„
                lt = max(1, np.random.normal(
                    config.LEAD_TIMES_MEAN[i], 
                    config.LEAD_TIMES_STD[i] * self.lead_time_uncertainty
                ))
                lead_times.append(max(1, int(np.round(lt))))
            return lead_times
        else:
            return config.LEAD_TIMES_MEAN.copy()
    
    def _generate_demand(self):
        """ê°œì„ ëœ ìˆ˜ìš” ìƒì„± (ë³€ë™ì„± ê°ì†Œ)"""
        base_demand = 50
        
        # ë” ì•ˆì •ì ì¸ ê³„ì ˆì„±
        seasonal_factor = 1 + 0.15 * np.sin(2 * np.pi * self.current_step / 52)
        if self.scenario == 'seasonal':
            seasonal_factor += self.seasonal_amplitude * np.sin(2 * np.pi * self.current_step / 12)
        
        # ì™„ë§Œí•œ íŠ¸ë Œë“œ
        trend_factor = 1 + 0.002 * self.current_step
        
        # ê°ì†Œëœ ë…¸ì´ì¦ˆ
        noise_factor = np.random.normal(1, self.demand_volatility)
        
        # ê¸‰ì¦ ì´ë²¤íŠ¸ ê°ì†Œ
        spike_probability = 0.02
        if self.scenario == 'high_volatility':
            spike_probability = 0.05
        
        spike_factor = np.random.lognormal(0, 0.3) if np.random.random() < spike_probability else 1
        
        # ë” ì•ˆì •ì ì¸ ì´ë™í‰ê· 
        if len(self.demand_history) > 5:
            ma_factor = 1 + 0.05 * (np.mean(list(self.demand_history)[-5:]) / 50 - 1)
        else:
            ma_factor = 1
        
        demand = max(10, base_demand * seasonal_factor * trend_factor * noise_factor * spike_factor * ma_factor)
        return demand
    
    def _get_observations(self):
        """í™•ì¥ëœ ê´€ì¸¡ ìƒíƒœ (ì •ë³´ ê³µìœ  í¬í•¨)"""
        observations = []
        current_demand = self._generate_demand()
        
        # ê¸€ë¡œë²Œ ë©”íŠ¸ë¦­ ê³„ì‚°
        self._update_global_metrics()
        
        # ìˆ˜ìš” ì˜ˆì¸¡ (ì´ë™í‰ê·  + íŠ¸ë Œë“œ)
        if len(self.demand_history) >= 10:
            recent_demands = list(self.demand_history)[-10:]
            demand_forecast = np.mean(recent_demands)
            demand_trend = np.polyfit(range(10), recent_demands, 1)[0]
        else:
            demand_forecast = current_demand
            demand_trend = 0
        
        for i in range(self.num_echelons):
            # ê¸°ë³¸ ê´€ì¸¡ê°’
            inventory_level = self.inventories[i] / config.INITIAL_INVENTORY[i]
            incoming_shipment = sum(self.in_transit[i]) / config.INITIAL_INVENTORY[i]
            demand_ratio = current_demand / 50.0
            lead_time_ratio = self.current_lead_times[i] / max(config.LEAD_TIMES_MEAN)
            
            # ì£¼ë¬¸ í‰í™œí™” ë©”íŠ¸ë¦­
            if len(self.order_history[i]) > 1:
                order_volatility = np.std(list(self.order_history[i])) / (np.mean(list(self.order_history[i])) + 1)
            else:
                order_volatility = 0
            
            # ìƒë¥˜/í•˜ë¥˜ ì •ë³´ (ì •ë³´ ê³µìœ )
            if config.INFORMATION_SHARING:
                if i < self.num_echelons - 1:
                    upstream_inventory = self.inventories[i+1] / config.INITIAL_INVENTORY[i+1]
                    upstream_order = self.previous_orders[i+1] / config.INITIAL_INVENTORY[i+1] if i+1 < len(self.previous_orders) else 0
                else:
                    upstream_inventory = 1.0
                    upstream_order = 0
                
                if i > 0:
                    downstream_inventory = self.inventories[i-1] / config.INITIAL_INVENTORY[i-1]
                    downstream_order = self.previous_orders[i-1] / config.INITIAL_INVENTORY[i-1] if i-1 >= 0 else 0
                else:
                    downstream_inventory = inventory_level
                    downstream_order = 0
            else:
                upstream_inventory = upstream_order = downstream_inventory = downstream_order = 0
            
            # ìˆ˜ìš” ì˜ˆì¸¡ ì •ë³´
            forecast_ratio = demand_forecast / 50.0
            trend_indicator = np.tanh(demand_trend)  # ì •ê·œí™”ëœ íŠ¸ë Œë“œ
            
            # ê¸€ë¡œë²Œ ê³µê¸‰ë§ ìƒíƒœ
            global_efficiency = self.supply_chain_efficiency
            
            obs = np.array([
                inventory_level,
                incoming_shipment,
                demand_ratio,
                lead_time_ratio,
                order_volatility,
                upstream_inventory,
                upstream_order,
                downstream_inventory,
                downstream_order,
                forecast_ratio,
                trend_indicator,
                global_efficiency
            ])
            observations.append(obs)
        
        return observations
    
    def _update_global_metrics(self):
        """ê¸€ë¡œë²Œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        # ì „ì²´ ì¬ê³  ë¹„ìœ¨
        total_inventory = np.sum(self.inventories)
        target_inventory = np.sum(config.INITIAL_INVENTORY)
        self.global_inventory_ratio = total_inventory / target_inventory
        
        # ê³µê¸‰ë§ íš¨ìœ¨ì„± (ì¬ê³  íšŒì „ìœ¨ + ì„œë¹„ìŠ¤ ë ˆë²¨)
        if len(self.total_costs) > 0:
            avg_cost = np.mean(self.total_costs[-10:])
            avg_inventory = np.mean([np.sum(inv) for inv in self.inventory_levels[-10:]])
            turnover = avg_cost / (avg_inventory + 1)
            
            shortage_rate = np.mean(self.shortage_events[-10:]) if len(self.shortage_events) > 0 else 0
            service_level = 1 - shortage_rate / self.num_echelons
            
            self.supply_chain_efficiency = 0.5 * turnover + 0.5 * service_level
        else:
            self.supply_chain_efficiency = 0.5
    
    def step(self, actions):
        """ê°œì„ ëœ í™˜ê²½ ìŠ¤í… (Bullwhip ì™„í™”)"""
        self.current_step += 1
        
        # í–‰ë™ì„ ì£¼ë¬¸ëŸ‰ìœ¼ë¡œ ë³€í™˜ (ë” ì•ˆì •ì ì¸ ë³€í™˜)
        orders = []
        for i, action in enumerate(actions):
            # ë” ë³´ìˆ˜ì ì¸ ì£¼ë¬¸ëŸ‰ ê³„ì‚°
            sigmoid_action = torch.sigmoid(torch.tensor(action[0])).item()
            
            # ìˆ˜ìš” ê¸°ë°˜ ì£¼ë¬¸ëŸ‰ ì¡°ì •
            if len(self.demand_history) > 0:
                recent_demand = np.mean(list(self.demand_history)[-3:])
                demand_factor = recent_demand / 50.0
            else:
                demand_factor = 1.0
            
            # ì¬ê³  ìˆ˜ì¤€ ê¸°ë°˜ ì¡°ì •
            inventory_factor = max(0.3, 1 - self.inventories[i] / config.INITIAL_INVENTORY[i])
            
            order_qty = sigmoid_action * config.MAX_ORDER_QTY[i] * demand_factor * inventory_factor
            orders.append(max(0, order_qty))
        
        orders = np.array(orders)
        
        # ì£¼ë¬¸ ì´ë ¥ ì—…ë°ì´íŠ¸
        for i, order in enumerate(orders):
            self.order_history[i].append(order)
        
        # í˜„ì¬ ìˆ˜ìš”
        current_demand = self._generate_demand()
        self.demand_history.append(current_demand)
        self.global_demand_forecast.append(current_demand)
        
        # ë¦¬ë“œíƒ€ì„ ì—…ë°ì´íŠ¸
        self.current_lead_times = self._sample_lead_times()
        
        # ì…ê³  ì²˜ë¦¬
        for i in range(self.num_echelons):
            if len(self.in_transit[i]) > 0:
                arrived_qty = self.in_transit[i].popleft()
                self.inventories[i] += arrived_qty
        
        # ì£¼ë¬¸ ì²˜ë¦¬
        shortages = np.zeros(self.num_echelons)
        
        # ì†Œë§¤ì  ìˆ˜ìš” ì²˜ë¦¬
        if self.inventories[0] >= current_demand:
            self.inventories[0] -= current_demand
        else:
            shortages[0] = current_demand - self.inventories[0]
            self.inventories[0] = 0
        
        # ê³„ì¸µ ê°„ ì£¼ë¬¸ ì²˜ë¦¬
        for i in range(self.num_echelons - 1):
            order_demand = orders[i]
            
            if self.inventories[i + 1] >= order_demand:
                self.inventories[i + 1] -= order_demand
                # ì…ê³  ìŠ¤ì¼€ì¤„ë§ (ë¦¬ë“œíƒ€ì„ ê³ ë ¤)
                while len(self.in_transit[i]) < self.current_lead_times[i]:
                    self.in_transit[i].append(0)
                self.in_transit[i].append(order_demand)
            else:
                shortages[i + 1] = order_demand - self.inventories[i + 1]
                delivered_qty = self.inventories[i + 1]
                self.inventories[i + 1] = 0
                
                # ë¶€ë¶„ ë°°ì†¡
                while len(self.in_transit[i]) < self.current_lead_times[i]:
                    self.in_transit[i].append(0)
                self.in_transit[i].append(delivered_qty)
        
        # ì œì¡°ì—…ì²´ ìƒì‚° (ë¬´ì œí•œ ê³µê¸‰ ê°€ì •)
        production_qty = orders[-1]
        while len(self.in_transit[-1]) < self.current_lead_times[-1]:
            self.in_transit[-1].append(0)
        self.in_transit[-1].append(production_qty)
        
        # ë³´ìƒ ê³„ì‚°
        rewards = self._calculate_rewards(orders, shortages, current_demand)
        
        # ì„±ê³¼ ì¶”ì  ì—…ë°ì´íŠ¸
        self._update_performance_tracking(orders, shortages, current_demand)
        
        self.previous_orders = orders.copy()
        
        # ì¢…ë£Œ ì¡°ê±´
        done = self.current_step >= config.MAX_STEPS
        
        return self._get_observations(), rewards, done, self._get_info()
    
    def _calculate_rewards(self, orders, shortages, current_demand):
        """ê°œì„ ëœ ë³´ìƒ í•¨ìˆ˜ (Bullwhip ì™„í™” ì´ˆì )"""
        rewards = []
        
        # Bullwhip ë©”íŠ¸ë¦­ ê³„ì‚°
        bullwhip_penalty = self._calculate_bullwhip_penalty()
        
        # í˜‘ë ¥ ë³´ë„ˆìŠ¤ ê³„ì‚°
        collaboration_bonus = self._calculate_collaboration_bonus()
        
        for i in range(self.num_echelons):
            # ê¸°ë³¸ ë¹„ìš©
            holding_cost = config.HOLDING_COSTS[i] * self.inventories[i]
            order_cost = config.ORDER_COSTS[i] * orders[i]
            shortage_cost = config.SHORTAGE_COSTS[i] * shortages[i]
            
            # ì£¼ë¬¸ í‰í™œí™” ë³´ë„ˆìŠ¤
            smoothing_bonus = 0
            if len(self.order_history[i]) > 3:
                recent_orders = list(self.order_history[i])[-4:]
                order_stability = 1 / (1 + np.std(recent_orders))
                smoothing_bonus = order_stability * 5.0
            
            # ì¬ê³  ìµœì í™” ë³´ë„ˆìŠ¤
            target_inventory = config.INITIAL_INVENTORY[i]
            inventory_deviation = abs(self.inventories[i] - target_inventory) / target_inventory
            inventory_bonus = max(0, 3 - inventory_deviation * 10)
            
            # ì„œë¹„ìŠ¤ ë ˆë²¨ ë³´ë„ˆìŠ¤
            service_bonus = 0
            if i == 0:  # ì†Œë§¤ì 
                if shortages[i] == 0:
                    service_bonus = 10
                else:
                    service_bonus = -5
            
            # ê³„ì¸µë³„ ê°€ì¤‘ ê¸°ë³¸ ë³´ìƒ
            base_reward = -(holding_cost + order_cost + shortage_cost)
            weighted_reward = base_reward * config.LAYER_WEIGHTS[i]
            
            # ì¢…í•© ë³´ìƒ
            total_reward = (weighted_reward + smoothing_bonus + inventory_bonus + 
                          service_bonus + collaboration_bonus - bullwhip_penalty)
            
            rewards.append(total_reward)
        
        return np.array(rewards)
    
    def _calculate_bullwhip_penalty(self):
        """Bullwhip Effect í˜ë„í‹° ê³„ì‚°"""
        if self.current_step < 10:
            return 0
        
        bullwhip_ratio = 0
        demand_variance = np.var(list(self.demand_history)[-10:])
        
        for i in range(self.num_echelons):
            if len(self.order_history[i]) >= 10:
                order_variance = np.var(list(self.order_history[i])[-10:])
                if demand_variance > 0:
                    layer_bullwhip = order_variance / demand_variance
                    bullwhip_ratio += layer_bullwhip * (i + 1)  # ìƒë¥˜ë¡œ ê°ˆìˆ˜ë¡ ê°€ì¤‘ì¹˜ ì¦ê°€
        
        # Bullwhip ë¹„ìœ¨ì´ 1ë³´ë‹¤ í¬ë©´ í˜ë„í‹°
        if bullwhip_ratio > self.num_echelons:
            penalty = (bullwhip_ratio - self.num_echelons) * config.BULLWHIP_PENALTY_WEIGHT
        else:
            penalty = 0
        
        self.bullwhip_metrics.append(bullwhip_ratio)
        return penalty
    
    def _calculate_collaboration_bonus(self):
        """í˜‘ë ¥ ë³´ë„ˆìŠ¤ ê³„ì‚°"""
        if not config.INFORMATION_SHARING:
            return 0
        
        bonus = 0
        
        # ì¬ê³  ê· í˜• ë³´ë„ˆìŠ¤
        inventory_balance = 1 - np.std(self.inventories / config.INITIAL_INVENTORY)
        bonus += inventory_balance * config.COLLABORATION_BONUS
        
        # ì£¼ë¬¸ íŒ¨í„´ ì¼ê´€ì„± ë³´ë„ˆìŠ¤
        if all(len(oh) >= 5 for oh in self.order_history):
            order_correlations = []
            for i in range(self.num_echelons - 1):
                orders_i = list(self.order_history[i])[-5:]
                orders_j = list(self.order_history[i + 1])[-5:]
                correlation = np.corrcoef(orders_i, orders_j)[0, 1]
                if not np.isnan(correlation):
                    order_correlations.append(abs(correlation))
            
            if order_correlations:
                avg_correlation = np.mean(order_correlations)
                bonus += avg_correlation * config.COLLABORATION_BONUS * 0.5
        
        self.collaboration_scores.append(bonus)
        return bonus
    
    def _update_performance_tracking(self, orders, shortages, current_demand):
        """ì„±ê³¼ ì¶”ì  ì—…ë°ì´íŠ¸"""
        # ì´ ë¹„ìš©
        total_cost = 0
        for i in range(self.num_echelons):
            holding_cost = config.HOLDING_COSTS[i] * self.inventories[i]
            order_cost = config.ORDER_COSTS[i] * orders[i]
            shortage_cost = config.SHORTAGE_COSTS[i] * shortages[i]
            total_cost += holding_cost + order_cost + shortage_cost
        
        self.total_costs.append(total_cost)
        self.inventory_levels.append(self.inventories.copy())
        self.shortage_events.append(np.sum(shortages > 0))
    
    def _get_info(self):
        """í™˜ê²½ ì •ë³´ ë°˜í™˜"""
        info = {
            'current_step': self.current_step,
            'inventories': self.inventories.copy(),
            'lead_times': self.current_lead_times.copy(),
            'demand': self.demand_history[-1] if self.demand_history else 0,
            'total_cost': self.total_costs[-1] if self.total_costs else 0,
            'bullwhip_metric': self.bullwhip_metrics[-1] if self.bullwhip_metrics else 0,
            'collaboration_score': self.collaboration_scores[-1] if self.collaboration_scores else 0,
            'global_inventory_ratio': self.global_inventory_ratio,
            'supply_chain_efficiency': self.supply_chain_efficiency
        }
        return info

class MultiAgentTrainer:
    """Multi-Agent SAC í›ˆë ¨ê¸°"""
    def __init__(self, env):
        self.env = env
        self.agents = []
        
        # ì—ì´ì „íŠ¸ ì´ˆê¸°í™”
        for i in range(config.NUM_ECHELONS):
            agent = MultiAgentSACAgent(
                agent_id=i,
                local_state_dim=env.local_state_dim,
                global_state_dim=env.global_state_dim,
                action_dim=env.action_dim,
                joint_action_dim=env.joint_action_dim
            )
            self.agents.append(agent)
        
        # ê³µìœ  ê²½í—˜ ì €ì¥ì†Œ
        self.shared_buffer = ReplayBuffer(config.BUFFER_SIZE * 2)
        
        # ì„±ê³¼ ì¶”ì 
        self.episode_rewards = []
        self.episode_costs = []
        self.bullwhip_ratios = []
        self.service_levels = []
        self.training_losses = {i: [] for i in range(config.NUM_ECHELONS)}
    
    def train(self):
        """í›ˆë ¨ ë£¨í”„"""
        print("Starting Multi-Agent SAC Training for Enhanced FMCG Supply Chain...")
        print(f"Configuration: {config.NUM_EPISODES} episodes, {config.MAX_STEPS} steps per episode")
        print(f"Bullwhip mitigation features: {'Enabled' if config.INFORMATION_SHARING else 'Disabled'}")
        
        for episode in range(config.NUM_EPISODES):
            states = self.env.reset()
            episode_rewards = np.zeros(config.NUM_ECHELONS)
            episode_losses = {i: [] for i in range(config.NUM_ECHELONS)}
            
            for step in range(config.MAX_STEPS):
                # í–‰ë™ ì„ íƒ
                actions = []
                for i, agent in enumerate(self.agents):
                    action = agent.select_action(states[i], evaluate=False)
                    actions.append([action])
                
                # í™˜ê²½ ìŠ¤í…
                next_states, rewards, done, info = self.env.step(actions)
                
                # ê²½í—˜ ì €ì¥
                for i, agent in enumerate(self.agents):
                    agent.replay_buffer.push(
                        states[i], actions[i], rewards[i], next_states[i], done
                    )
                    
                    # ê³µìœ  ë²„í¼ì—ë„ ì €ì¥
                    self.shared_buffer.push(
                        states[i], actions[i], rewards[i], next_states[i], done
                    )
                
                # ì—ì´ì „íŠ¸ ì—…ë°ì´íŠ¸
                for i, agent in enumerate(self.agents):
                    if len(agent.replay_buffer) > config.BATCH_SIZE:
                        losses = agent.update(self.shared_buffer)
                        if losses:
                            for key, value in losses.items():
                                episode_losses[i].append(value)
                
                episode_rewards += rewards
                states = next_states
                
                if done:
                    break
            
            # ì—í”¼ì†Œë“œ ì¢…ë£Œ í›„ ê¸°ë¡
            self.episode_rewards.append(episode_rewards.mean())
            self.episode_costs.append(self.env.total_costs[-1] if self.env.total_costs else 0)
            
            if self.env.bullwhip_metrics:
                self.bullwhip_ratios.append(self.env.bullwhip_metrics[-1])
            
            # ì„œë¹„ìŠ¤ ë ˆë²¨ ê³„ì‚°
            service_level = 1 - (self.env.shortage_events[-1] / config.NUM_ECHELONS) if self.env.shortage_events else 1.0
            self.service_levels.append(service_level)
            
            # ì†ì‹¤ ê¸°ë¡
            for i in range(config.NUM_ECHELONS):
                if episode_losses[i]:
                    self.training_losses[i].append(np.mean(episode_losses[i]))
                else:
                    self.training_losses[i].append(0)
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            if (episode + 1) % 50 == 0:
                avg_reward = np.mean(self.episode_rewards[-50:])
                avg_cost = np.mean(self.episode_costs[-50:])
                avg_bullwhip = np.mean(self.bullwhip_ratios[-50:]) if self.bullwhip_ratios else 0
                avg_service = np.mean(self.service_levels[-50:])
                
                print(f"Episode {episode + 1}/{config.NUM_EPISODES}")
                print(f"  Avg Reward: {avg_reward:.2f}")
                print(f"  Avg Cost: {avg_cost:.2f}")
                print(f"  Avg Bullwhip Ratio: {avg_bullwhip:.3f}")
                print(f"  Avg Service Level: {avg_service:.3f}")
                print(f"  Alpha values: {[agent.alpha.item() for agent in self.agents]}")
        
        print("Training completed!")
        return self.agents
    
    def evaluate(self, agents, num_episodes=10):
        """í›ˆë ¨ëœ ì—ì´ì „íŠ¸ í‰ê°€"""
        print(f"\nEvaluating trained agents over {num_episodes} episodes...")
        
        eval_rewards = []
        eval_costs = []
        eval_bullwhip = []
        eval_service = []
        
        for episode in range(num_episodes):
            states = self.env.reset()
            episode_reward = 0
            
            for step in range(config.MAX_STEPS):
                actions = []
                for i, agent in enumerate(agents):
                    action = agent.select_action(states[i], evaluate=True)
                    actions.append([action])
                
                states, rewards, done, info = self.env.step(actions)
                episode_reward += rewards.mean()
                
                if done:
                    break
            
            eval_rewards.append(episode_reward)
            eval_costs.append(info['total_cost'])
            eval_bullwhip.append(info['bullwhip_metric'])
            
            # ì„œë¹„ìŠ¤ ë ˆë²¨
            service_level = 1 - (self.env.shortage_events[-1] / config.NUM_ECHELONS) if self.env.shortage_events else 1.0
            eval_service.append(service_level)
        
        # í‰ê°€ ê²°ê³¼
        results = {
            'avg_reward': np.mean(eval_rewards),
            'std_reward': np.std(eval_rewards),
            'avg_cost': np.mean(eval_costs),
            'std_cost': np.std(eval_costs),
            'avg_bullwhip': np.mean(eval_bullwhip),
            'std_bullwhip': np.std(eval_bullwhip),
            'avg_service_level': np.mean(eval_service),
            'std_service_level': np.std(eval_service)
        }
        
        print(f"Evaluation Results:")
        print(f"  Average Reward: {results['avg_reward']:.2f} Â± {results['std_reward']:.2f}")
        print(f"  Average Cost: {results['avg_cost']:.2f} Â± {results['std_cost']:.2f}")
        print(f"  Average Bullwhip Ratio: {results['avg_bullwhip']:.3f} Â± {results['std_bullwhip']:.3f}")
        print(f"  Average Service Level: {results['avg_service_level']:.3f} Â± {results['std_service_level']:.3f}")
        
        return results

def plot_training_results(trainer):
    """í›ˆë ¨ ê²°ê³¼ ì‹œê°í™”"""
    fig, axes = plt.subplots(2, 3, figsize=(18, 12))
    fig.suptitle('Enhanced FMCG Supply Chain Training Results (Bullwhip Mitigation)', fontsize=16)
    
    # ë³´ìƒ ì¶”ì´
    axes[0, 0].plot(trainer.episode_rewards)
    axes[0, 0].set_title('Episode Rewards')
    axes[0, 0].set_xlabel('Episode')
    axes[0, 0].set_ylabel('Average Reward')
    axes[0, 0].grid(True)
    
    # ë¹„ìš© ì¶”ì´
    axes[0, 1].plot(trainer.episode_costs)
    axes[0, 1].set_title('Episode Costs')
    axes[0, 1].set_xlabel('Episode')
    axes[0, 1].set_ylabel('Total Cost')
    axes[0, 1].grid(True)
    
    # Bullwhip ë¹„ìœ¨
    if trainer.bullwhip_ratios:
        axes[0, 2].plot(trainer.bullwhip_ratios)
        axes[0, 2].axhline(y=config.NUM_ECHELONS, color='r', linestyle='--', 
                          label=f'Target (â‰¤{config.NUM_ECHELONS})')
        axes[0, 2].set_title('Bullwhip Ratio')
        axes[0, 2].set_xlabel('Episode')
        axes[0, 2].set_ylabel('Bullwhip Ratio')
        axes[0, 2].legend()
        axes[0, 2].grid(True)
    
    # ì„œë¹„ìŠ¤ ë ˆë²¨
    axes[1, 0].plot(trainer.service_levels)
    axes[1, 0].axhline(y=0.95, color='g', linestyle='--', label='Target (95%)')
    axes[1, 0].set_title('Service Level')
    axes[1, 0].set_xlabel('Episode')
    axes[1, 0].set_ylabel('Service Level')
    axes[1, 0].legend()
    axes[1, 0].grid(True)
    
    # í›ˆë ¨ ì†ì‹¤ (í‰ê· )
    avg_losses = []
    for episode in range(len(trainer.training_losses[0])):
        episode_avg = np.mean([trainer.training_losses[i][episode] for i in range(config.NUM_ECHELONS)])
        avg_losses.append(episode_avg)
    
    axes[1, 1].plot(avg_losses)
    axes[1, 1].set_title('Average Training Loss')
    axes[1, 1].set_xlabel('Episode')
    axes[1, 1].set_ylabel('Loss')
    axes[1, 1].grid(True)
    
    # ì„±ê³¼ ê°œì„  (ì´ë™í‰ê· )
    window = 50
    if len(trainer.episode_rewards) >= window:
        moving_rewards = pd.Series(trainer.episode_rewards).rolling(window).mean()
        moving_costs = pd.Series(trainer.episode_costs).rolling(window).mean()
        
        ax2 = axes[1, 2]
        ax2.plot(moving_rewards, 'b-', label='Reward (MA)')
        ax2.set_ylabel('Reward', color='b')
        ax2.tick_params(axis='y', labelcolor='b')
        
        ax3 = ax2.twinx()
        ax3.plot(moving_costs, 'r-', label='Cost (MA)')
        ax3.set_ylabel('Cost', color='r')
        ax3.tick_params(axis='y', labelcolor='r')
        
        ax2.set_title('Performance Improvement (Moving Average)')
        ax2.set_xlabel('Episode')
        ax2.grid(True)
    
    plt.tight_layout()
    plt.show()

def run_comparison_scenarios():
    """ì‹œë‚˜ë¦¬ì˜¤ë³„ ë¹„êµ ì‹¤í—˜ (SEASONAL & HIGH_VOLATILITYë§Œ)"""
    # ì‹œë‚˜ë¦¬ì˜¤ë¥¼ seasonalê³¼ high_volatilityë§Œìœ¼ë¡œ ì œí•œ
    scenarios = ['seasonal', 'high_volatility']
    results = {}
    
    print("Running scenario comparison experiments (SEASONAL & HIGH_VOLATILITY only)...")
    
    for scenario in scenarios:
        print(f"\n=== Testing Scenario: {scenario.upper()} ===")
        
        # í™˜ê²½ ë° í›ˆë ¨ê¸° ìƒì„±
        env = EnhancedFMCGSupplyChain(scenario=scenario)
        trainer = MultiAgentTrainer(env)
        
        # ì¶•ì•½ëœ í›ˆë ¨ (ë¹„êµìš©)
        original_episodes = config.NUM_EPISODES
        config.NUM_EPISODES = 300  # ë¹ ë¥¸ ë¹„êµë¥¼ ìœ„í•´ ê°ì†Œ
        
        # í›ˆë ¨
        trained_agents = trainer.train()
        
        # í‰ê°€
        eval_results = trainer.evaluate(trained_agents, num_episodes=20)
        results[scenario] = eval_results
        
        # ì›ë˜ ì„¤ì • ë³µì›
        config.NUM_EPISODES = original_episodes
    
    # ê²°ê³¼ ë¹„êµ ì¶œë ¥
    print("\n" + "="*80)
    print("SCENARIO COMPARISON RESULTS (SEASONAL & HIGH_VOLATILITY)")
    print("="*80)
    
    comparison_df = pd.DataFrame(results).T
    print(comparison_df.round(3))
    
    # ì‹œê°í™”
    fig, axes = plt.subplots(2, 2, figsize=(15, 10))
    fig.suptitle('Scenario Comparison: SEASONAL vs HIGH_VOLATILITY', fontsize=16)
    
    scenarios_list = list(results.keys())
    
    # í‰ê·  ë³´ìƒ
    rewards = [results[s]['avg_reward'] for s in scenarios_list]
    axes[0, 0].bar(scenarios_list, rewards, color=['skyblue', 'salmon'])
    axes[0, 0].set_title('Average Reward by Scenario')
    axes[0, 0].set_ylabel('Average Reward')
    
    # í‰ê·  ë¹„ìš©
    costs = [results[s]['avg_cost'] for s in scenarios_list]
    axes[0, 1].bar(scenarios_list, costs, color=['lightgreen', 'orange'])
    axes[0, 1].set_title('Average Cost by Scenario')
    axes[0, 1].set_ylabel('Average Cost')
    
    # Bullwhip ë¹„ìœ¨
    bullwhip = [results[s]['avg_bullwhip'] for s in scenarios_list]
    axes[1, 0].bar(scenarios_list, bullwhip, color=['gold', 'lightcoral'])
    axes[1, 0].axhline(y=config.NUM_ECHELONS, color='r', linestyle='--', 
                      label=f'Target (â‰¤{config.NUM_ECHELONS})')
    axes[1, 0].set_title('Average Bullwhip Ratio by Scenario')
    axes[1, 0].set_ylabel('Bullwhip Ratio')
    axes[1, 0].legend()
    
    # ì„œë¹„ìŠ¤ ë ˆë²¨
    service = [results[s]['avg_service_level'] for s in scenarios_list]
    axes[1, 1].bar(scenarios_list, service, color=['mediumpurple', 'lightsteelblue'])
    axes[1, 1].axhline(y=0.95, color='g', linestyle='--', label='Target (95%)')
    axes[1, 1].set_title('Average Service Level by Scenario')
    axes[1, 1].set_ylabel('Service Level')
    axes[1, 1].legend()
    
    plt.tight_layout()
    plt.show()
    
    return results

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (ìˆ˜ì •ë¨)
def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("Enhanced FMCG Multi-Echelon Supply Chain with Bullwhip Mitigation")
    print("="*80)
    print(f"Key Features:")
    print(f"- Information Sharing: {config.INFORMATION_SHARING}")
    print(f"- Order Smoothing Factor: {config.ORDER_SMOOTHING_FACTOR}")
    print(f"- Bullwhip Penalty Weight: {config.BULLWHIP_PENALTY_WEIGHT}")
    print(f"- Collaboration Bonus: {config.COLLABORATION_BONUS}")
    print(f"- Demand Volatility: {config.DEMAND_VOLATILITY}")
    print("="*80)
    
    # ì‹œë‚˜ë¦¬ì˜¤ ë¹„êµë§Œ ì‹¤í–‰ (SEASONAL & HIGH_VOLATILITY)
    print("Running SEASONAL vs HIGH_VOLATILITY comparison...")
    scenario_results = run_comparison_scenarios()
    
    print("\n" + "="*80)
    print("EXPERIMENT COMPLETED SUCCESSFULLY!")
    print("="*80)
    
    # ë‘ ì‹œë‚˜ë¦¬ì˜¤ ê²°ê³¼ ë¹„êµ ìš”ì•½
    print(f"\nScenario Comparison Summary:")
    print(f"SEASONAL Scenario:")
    print(f"  - Average Reward: {scenario_results['seasonal']['avg_reward']:.2f}")
    print(f"  - Average Cost: {scenario_results['seasonal']['avg_cost']:.2f}")
    print(f"  - Average Bullwhip Ratio: {scenario_results['seasonal']['avg_bullwhip']:.3f}")
    print(f"  - Average Service Level: {scenario_results['seasonal']['avg_service_level']:.3f}")
    
    print(f"\nHIGH_VOLATILITY Scenario:")
    print(f"  - Average Reward: {scenario_results['high_volatility']['avg_reward']:.2f}")
    print(f"  - Average Cost: {scenario_results['high_volatility']['avg_cost']:.2f}")
    print(f"  - Average Bullwhip Ratio: {scenario_results['high_volatility']['avg_bullwhip']:.3f}")
    print(f"  - Average Service Level: {scenario_results['high_volatility']['avg_service_level']:.3f}")
    
    # ìŠ¹ì íŒë³„
    seasonal_score = scenario_results['seasonal']['avg_service_level'] - scenario_results['seasonal']['avg_bullwhip']/10
    volatility_score = scenario_results['high_volatility']['avg_service_level'] - scenario_results['high_volatility']['avg_bullwhip']/10
    
    print(f"\nOverall Performance Winner:")
    if seasonal_score > volatility_score:
        print("ğŸ† SEASONAL scenario performed better overall!")
    else:
        print("ğŸ† HIGH_VOLATILITY scenario performed better overall!")

if __name__ == "__main__":
    main()
